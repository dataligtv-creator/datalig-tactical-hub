import streamlit as st
import google.generativeai as genai
from pinecone import Pinecone
# KRÄ°TÄ°K: Colab'deki motorun aynÄ±sÄ±nÄ± kullanmak zorundayÄ±z
from langchain_community.embeddings import HuggingFaceEmbeddings

# --- SAYFA AYARLARI ---
st.set_page_config(page_title="Regista Tactical Hub", page_icon="âš½", layout="wide")

# --- GÃ–RSELLÄ°K ---
st.markdown("""
<style>
    .main {background-color: #0e1117;}
    h1 {color: #ff4b4b;}
    .stChatMessage {border-radius: 10px;}
</style>
""", unsafe_allow_html=True)

st.title("âš½ Regista Tactical Hub")
st.caption("AI Destekli Taktik Analiz & ArÅŸiv UzmanÄ± (Powered by Gemini 2.5)")

# --- API KURULUMLARI ---
if "GOOGLE_API_KEY" in st.secrets and "PINECONE_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    
    try:
        pc = Pinecone(api_key=st.secrets["PINECONE_API_KEY"])
        index_name = "regista-arsiv"
        pinecone_index = pc.Index(index_name)
        
        # --- MOTOR DEÄÄ°ÅÄ°KLÄ°ÄÄ° ---
        # Colab'de "all-MiniLM-L6-v2" kullandÄ±k.
        # Streamlit'te de AYNISINI kullanmazsak arÅŸivden saÃ§ma sonuÃ§lar dÃ¶ner.
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        
        db_status = "ğŸŸ¢ ArÅŸiv BaÄŸlÄ± (HuggingFace Motoru)"
    except Exception as e:
        pinecone_index = None
        db_status = f"ğŸ”´ ArÅŸiv HatasÄ±: {e}"
else:
    st.error("ğŸš¨ API AnahtarlarÄ± Eksik!")
    st.stop()

# --- YAN MENÃœ ---
with st.sidebar:
    st.header("Saha KenarÄ±")
    st.info(f"Durum: {db_status}")
    st.info("Model: Gemini 2.5") # Bilgi ekranÄ±
    st.markdown("---")
    st.markdown("**NasÄ±l KullanÄ±lÄ±r?**")
    st.markdown("1. Sorunu yaz.")
    st.markdown("2. Sistem arÅŸivden tarayÄ±p cevaplar.")

# --- SOHBET ---
if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Gemini 2.5 motoru Ä±sÄ±ndÄ± hocam. ArÅŸivi taramaya hazÄ±rÄ±z."}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- ARÅÄ°V FONKSÄ°YONU ---
def arsivden_bul(soru):
    if not pinecone_index:
        return None, []
    
    try:
        # Soruyu vektÃ¶re Ã§evir (HuggingFace ile)
        soru_vektor = embeddings.embed_query(soru)
        
        # Pinecone'da ara
        sonuc = pinecone_index.query(
            vector=soru_vektor,
            top_k=3,
            include_metadata=True
        )
        metinler = ""
        kaynaklar = []
        for match in sonuc['matches']:
            if 'text' in match['metadata']:
                metinler += match['metadata']['text'] + "\n\n"
                src = match['metadata'].get('source', 'Bilinmeyen Dosya')
                kaynaklar.append(src)
        return metinler, list(set(kaynaklar))
    except Exception as e:
        return None, []

# --- SOHBET MANTIÄI ---
if prompt := st.chat_input("Sorunu yaz..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("ğŸ” *ArÅŸiv taranÄ±yor...*")
        
        context, kaynaklar = arsivden_bul(prompt)
        
        prompt_taslagi = """
        Sen uzman bir futbol analistisin.
        KullanÄ±cÄ± Sorusu: {soru}
        ArÅŸiv Bilgileri: {bilgi}
        
        EÄŸer arÅŸivde varsa oradan cevapla, yoksa kendi bilgini kullan.
        """
        
        if context:
            final_prompt = prompt_taslagi.format(soru=prompt, bilgi=context)
        else:
            final_prompt = prompt_taslagi.format(soru=prompt, bilgi="(ArÅŸivde bilgi yok)")

        try:
            # SENÄ°N Ä°STEÄÄ°N ÃœZERÄ°NE 2.5 MODELÄ°:
            # Not: EÄŸer tam ismi 'gemini-2.5-pro' ise burayÄ± dÃ¼zeltirsin.
            model = genai.GenerativeModel('gemini-2.5-flash') 
            response = model.generate_content(final_prompt)
            ai_response = response.text
            
            if kaynaklar:
                ai_response += "\n\n--- \nğŸ“š **Kaynaklar:**\n" + "\n".join([f"- {k}" for k in kaynaklar])
            
            message_placeholder.markdown(ai_response)
            st.session_state.messages.append({"role": "assistant", "content": ai_response})
            
        except Exception as e:
            st.error(f"Hata: {e}")
