import streamlit as st
import google.generativeai as genai
from pinecone import Pinecone
from langchain_community.embeddings import HuggingFaceEmbeddings
import time
import uuid

# --- SAYFA AYARLARI ---
st.set_page_config(
    page_title="DATALIG Pro Suite | Gemini 3",
    page_icon="âš½",
    layout="wide"
)

# --- ğŸ” GÄ°RÄ°Å KONTROLÃœ ---
if 'authenticated' not in st.session_state:
    st.session_state.authenticated = False

def check_login():
    if st.session_state.password == "datalig2025":
        st.session_state.authenticated = True
    else:
        st.session_state.login_error = "HatalÄ± ÅŸifre teknik direktÃ¶rÃ¼m!"

if not st.session_state.authenticated:
    # (Login arayÃ¼zÃ¼ kodun aynÄ± kalabilir, burayÄ± hÄ±zlÄ± geÃ§iyorum)
    st.text_input("Åifre", type="password", key="password", on_change=check_login)
    st.button("GiriÅŸ Yap", on_click=check_login)
    st.stop()

# --- ğŸš€ API & MODEL YAPILANDIRMASI (ARALIK 2025) ---
if "GOOGLE_API_KEY" in st.secrets and "PINECONE_API_KEY" in st.secrets:
    genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    
    @st.cache_resource
    def get_model():
        # ARALIK 2025 GÃœNCEL KODU: gemini-3-flash-preview
        # Bu model Ã¼st dÃ¼zey akÄ±l yÃ¼rÃ¼tme ve multimodal yeteneklere sahiptir.
        return genai.GenerativeModel(
            model_name='gemini-3-flash-preview',
            # CanlÄ± internet verisiyle halÃ¼sinasyonu engelleyen Grounding aracÄ±
            tools=[{"google_search": {}}] 
        )
    
    model = get_model()

    try:
        pc = Pinecone(api_key=st.secrets["PINECONE_API_KEY"])
        pinecone_index = pc.Index("regista-arsiv")
        # Embedding modelini gÃ¼ncel tutuyoruz
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        db_status, db_color = "ONLINE (GEMINI 3 READY)", "#00e5ff"
    except Exception:
        db_status, db_color = "OFFLINE", "#ef4444"
else:
    st.error("ğŸš¨ API KEY EKSÄ°K!")
    st.stop()

# --- SIDEBAR & SÄ°STEMÄ° EÄÄ°T (Aynen Kalabilir) ---
with st.sidebar:
    st.markdown(f"**SÄ°STEM DURUMU:** <span style='color:{db_color}'>{db_status}</span>", unsafe_allow_html=True)
    if st.button("ğŸ”’ Ã‡Ä±kÄ±ÅŸ Yap"):
        st.session_state.authenticated = False
        st.rerun()

# --- ANA EKRAN ---
st.markdown("### âš½ DATALIG <span style='color:#94a3b8;'>ORACLE V3.0 (Gemini 3 Flash)</span>", unsafe_allow_html=True)
st.markdown("---")

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "Sistem Gemini 3 Flash ile gÃ¼ncellendi hocam. Taktik tahtasÄ± emrinizde."}]

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# --- SORU-CEVAP MEKANÄ°ZMASI ---
if prompt := st.chat_input("Taktiksel analiz sorgusu..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        msg_placeholder = st.empty()
        
        with st.status("âš¡ DERÄ°N AKIL YÃœRÃœTME AKTÄ°F...", expanded=False):
            # 1. Pinecone'dan ArÅŸiv Verisini Ã‡ek
            soru_vektor = embeddings.embed_query(prompt)
            search_results = pinecone_index.query(vector=soru_vektor, top_k=10, include_metadata=True)
            context = "\n".join([res['metadata']['text'] for res in search_results['matches']])
            
            # 2. Gemini 3 Ä°Ã§in GeliÅŸmiÅŸ Prompt
            # 'thinking' (dÃ¼ÅŸÃ¼nme) Ã¶zelliÄŸini tetikleyen yapÄ±
            full_prompt = f"""
            TALÄ°MAT: Sen profesyonel futbol analisti 'DATALIG AI'sÄ±n. 
            ARÅÄ°V BÄ°LGÄ°LERÄ° (Pinecone): {context if context else "Ã–zel veri yok."}
            
            GÃ–REV: YukarÄ±daki arÅŸiv bilgilerini, kendi futbol bilginle ve Google Search Ã¼zerinden gelen 
            gÃ¼ncel dÃ¼nya verileriyle (sakatlÄ±klar, form durumu) birleÅŸtirerek derin bir analiz yap.
            
            SORU: {prompt}
            """

        try:
            # Gemini 3 Flash Ã¼retimi
            response = model.generate_content(full_prompt)
            ai_response = response.text
            
            msg_placeholder.markdown(ai_response)
            st.session_state.messages.append({"role": "assistant", "content": ai_response})
        except Exception as e:
            st.error(f"Sistem HatasÄ±: {e}. LÃ¼tfen model ismini kontrol edin.")
